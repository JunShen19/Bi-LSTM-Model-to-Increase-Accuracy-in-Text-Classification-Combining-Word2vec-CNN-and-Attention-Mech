{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data=pd.read_csv(r'D:\\駿燊\\nlpPractice\\IMDB Dataset.csv', encoding=\"unicode_escape\")\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DBLAB2020\\anaconda3\\envs\\nlpPractice\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review ha mention watch 1 oz episod youll ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one review ha mention watch 1 oz episod youll ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu veri unassu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonder littl product film techniqu veri unassu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought thi wa wonder way spend time hot summe...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought thi wa wonder way spend time hot summe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  one review ha mention watch 1 oz episod youll ...  positive   \n",
       "1  wonder littl product film techniqu veri unassu...  positive   \n",
       "2  thought thi wa wonder way spend time hot summe...  positive   \n",
       "3  basic famili littl boy jake think zombi hi clo...  negative   \n",
       "4  petter mattei love time money visual stun film...  positive   \n",
       "\n",
       "                                     cleaned_reviews  \n",
       "0  one review ha mention watch 1 oz episod youll ...  \n",
       "1  wonder littl product film techniqu veri unassu...  \n",
       "2  thought thi wa wonder way spend time hot summe...  \n",
       "3  basic famili littl boy jake think zombi hi clo...  \n",
       "4  petter mattei love time money visual stun film...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text) # Remove HTML from text\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n",
    "    text = [word for word in text.split() if word not in stop]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "imdb_data['cleaned_reviews'] = imdb_data['review'].apply(data_preprocessing)\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for text in imdb_data['cleaned_reviews'] for word in text.split()]\n",
    "count_words = Counter(corpus)\n",
    "sorted_words = count_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = []\n",
    "values = []\n",
    "for key, value in sorted_words[:20]:\n",
    "    keys.append(key)\n",
    "    values.append(value)\n",
    "    \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(keys, values)\n",
    "plt.title('Top 20 most common words', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 242, 8, 367, 16, 469, 2882, 202, 416, 1614, 127, 1, 561, 122, 5344, 37, 41, 2810, 2882, 3, 1101, 11153, 22, 549, 105, 127, 308, 31, 1389, 1, 23, 5008, 415, 8418, 1, 23, 590, 1712, 1091, 668, 340, 549, 2956, 250, 73, 42629, 178, 2882, 6185, 313, 10962, 5474, 1839, 532, 42630, 1112, 1291, 16965, 454, 3387, 1924, 790, 1929, 1815, 883, 248, 17598, 12572, 277, 3777, 3152, 454, 298, 60100, 3487, 9452, 5870, 1084, 934, 2174, 12573, 15357, 258, 2036, 5871, 426, 6717, 6439, 57, 167, 14913, 19, 46, 213, 810, 23, 612, 112, 207, 23, 505, 1494, 693, 115, 296, 1055, 2201, 194, 693, 638, 693, 60101, 84, 759, 117, 37, 202, 63, 141, 2810, 1390, 3, 1746, 360, 46, 3, 1434, 16, 380, 944, 2882, 124, 7710, 277, 493, 1215, 549, 549, 5245, 2957, 1755, 10380, 2579, 19089, 4382, 10380, 113, 477, 12, 182, 25, 1034, 707, 662, 4382, 109, 790, 4108, 612, 304, 635, 1001, 790, 363, 16, 2882, 130, 146, 1801, 2610, 60102, 12, 459, 3388, 399]]\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "reviews_int = []\n",
    "for text in imdb_data['cleaned_reviews']:\n",
    "    r = [vocab_to_int[word] for word in text.split()]\n",
    "    reviews_int.append(r)\n",
    "\n",
    "print(reviews_int[:1])\n",
    "imdb_data['Review int'] = reviews_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>Review int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review ha mention watch 1 oz episod youll ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one review ha mention watch 1 oz episod youll ...</td>\n",
       "      <td>[6, 242, 8, 367, 16, 469, 2882, 202, 416, 1614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu veri unassu...</td>\n",
       "      <td>1</td>\n",
       "      <td>wonder littl product film techniqu veri unassu...</td>\n",
       "      <td>[116, 59, 231, 4, 1622, 15, 13144, 15, 60103, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought thi wa wonder way spend time hot summe...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought thi wa wonder way spend time hot summe...</td>\n",
       "      <td>[110, 1, 3, 116, 40, 660, 9, 825, 1337, 2116, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
       "      <td>0</td>\n",
       "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
       "      <td>[402, 150, 59, 243, 2848, 35, 564, 5, 3204, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>1</td>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>[60107, 8419, 32, 9, 228, 528, 1086, 4, 16, 34...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  one review ha mention watch 1 oz episod youll ...          1   \n",
       "1  wonder littl product film techniqu veri unassu...          1   \n",
       "2  thought thi wa wonder way spend time hot summe...          1   \n",
       "3  basic famili littl boy jake think zombi hi clo...          0   \n",
       "4  petter mattei love time money visual stun film...          1   \n",
       "\n",
       "                                     cleaned_reviews  \\\n",
       "0  one review ha mention watch 1 oz episod youll ...   \n",
       "1  wonder littl product film techniqu veri unassu...   \n",
       "2  thought thi wa wonder way spend time hot summe...   \n",
       "3  basic famili littl boy jake think zombi hi clo...   \n",
       "4  petter mattei love time money visual stun film...   \n",
       "\n",
       "                                          Review int  \n",
       "0  [6, 242, 8, 367, 16, 469, 2882, 202, 416, 1614...  \n",
       "1  [116, 59, 231, 4, 1622, 15, 13144, 15, 60103, ...  \n",
       "2  [110, 1, 3, 116, 40, 660, 9, 825, 1337, 2116, ...  \n",
       "3  [402, 150, 59, 243, 2848, 35, 564, 5, 3204, 5,...  \n",
       "4  [60107, 8419, 32, 9, 228, 528, 1086, 4, 16, 34...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'] = imdb_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "imdb_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>Review int</th>\n",
       "      <th>Review len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review ha mention watch 1 oz episod youll ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one review ha mention watch 1 oz episod youll ...</td>\n",
       "      <td>[6, 242, 8, 367, 16, 469, 2882, 202, 416, 1614...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu veri unassu...</td>\n",
       "      <td>1</td>\n",
       "      <td>wonder littl product film techniqu veri unassu...</td>\n",
       "      <td>[116, 59, 231, 4, 1622, 15, 13144, 15, 60103, ...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought thi wa wonder way spend time hot summe...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought thi wa wonder way spend time hot summe...</td>\n",
       "      <td>[110, 1, 3, 116, 40, 660, 9, 825, 1337, 2116, ...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
       "      <td>0</td>\n",
       "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
       "      <td>[402, 150, 59, 243, 2848, 35, 564, 5, 3204, 5,...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>1</td>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>[60107, 8419, 32, 9, 228, 528, 1086, 4, 16, 34...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  one review ha mention watch 1 oz episod youll ...          1   \n",
       "1  wonder littl product film techniqu veri unassu...          1   \n",
       "2  thought thi wa wonder way spend time hot summe...          1   \n",
       "3  basic famili littl boy jake think zombi hi clo...          0   \n",
       "4  petter mattei love time money visual stun film...          1   \n",
       "\n",
       "                                     cleaned_reviews  \\\n",
       "0  one review ha mention watch 1 oz episod youll ...   \n",
       "1  wonder littl product film techniqu veri unassu...   \n",
       "2  thought thi wa wonder way spend time hot summe...   \n",
       "3  basic famili littl boy jake think zombi hi clo...   \n",
       "4  petter mattei love time money visual stun film...   \n",
       "\n",
       "                                          Review int  Review len  \n",
       "0  [6, 242, 8, 367, 16, 469, 2882, 202, 416, 1614...         175  \n",
       "1  [116, 59, 231, 4, 1622, 15, 13144, 15, 60103, ...          91  \n",
       "2  [110, 1, 3, 116, 40, 660, 9, 825, 1337, 2116, ...          94  \n",
       "3  [402, 150, 59, 243, 2848, 35, 564, 5, 3204, 5,...          67  \n",
       "4  [60107, 8419, 32, 9, 228, 528, 1086, 4, 16, 34...         130  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_len = [len(x) for x in reviews_int]\n",
    "imdb_data['Review len'] = review_len\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    50000.000000\n",
      "mean       128.203380\n",
      "std         95.040802\n",
      "min          3.000000\n",
      "25%         70.000000\n",
      "50%         96.000000\n",
      "75%        156.000000\n",
      "max       1483.000000\n",
      "Name: Review len, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(imdb_data['Review len'].describe())\n",
    "\n",
    "imdb_data['Review len'].hist()\n",
    "plt.title('Review length distribution', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding / Truncating the remaining data\n",
    "- To deal with both short and long reviews, we will pad or truncate all our reviews to a specific length. We define this length by Sequence Length. This sequence length is same as number of time steps for LSTM layer.\n",
    "\n",
    "- For reviews shorter than seq_length, we will pad with 0s. For reviews longer than seq_length we will truncate them to the first seq_length words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Padding(review_int, seq_len):\n",
    "    '''\n",
    "    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(review_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     6   242     8   367    16   469  2882   202   416  1614   127\n",
      "     1   561   122  5344    37    41  2810  2882     3  1101 11153    22\n",
      "   549   105   127   308    31  1389     1    23  5008   415  8418     1\n",
      "    23   590  1712  1091   668   340   549  2956   250    73 42629   178\n",
      "  2882  6185   313 10962  5474  1839   532 42630  1112  1291 16965   454\n",
      "  3387  1924   790  1929  1815   883   248 17598 12572   277  3777  3152\n",
      "   454   298 60100  3487  9452  5870  1084   934  2174 12573 15357   258\n",
      "  2036  5871   426  6717  6439    57   167 14913    19    46   213   810\n",
      "    23   612   112   207    23   505  1494   693   115   296  1055  2201\n",
      "   194   693   638   693 60101    84   759   117    37   202    63   141\n",
      "  2810  1390     3  1746   360    46     3  1434    16   380   944  2882\n",
      "   124  7710   277   493  1215   549   549  5245  2957  1755 10380  2579\n",
      " 19089  4382 10380   113   477    12   182    25  1034   707   662  4382\n",
      "   109   790  4108   612   304   635  1001   790   363    16  2882   130\n",
      "   146  1801  2610 60102    12   459  3388   399]\n"
     ]
    }
   ],
   "source": [
    "features = Padding(reviews_int, 200)\n",
    "print(features[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, Validation, Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_remain, y_train, y_remain = train_test_split(features, imdb_data['sentiment'].to_numpy(), test_size=0.2, random_state=1)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Dataloaders and Batching\n",
    "- After creating our training, test and validation data. Next step is to create dataloaders for this data. We can use generator function for batching our data into batches instead we will use a TensorDataset. This is one of a very useful utility in PyTorch for using our data with DataLoaders with exact same ease as of torchvision datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor dataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,    54,   901,  3587],\n",
      "        [    0,     0,     0,  ...,   323,  2039,    16],\n",
      "        [    0,     0,     0,  ...,    13,     1,     2],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,   732,  2169, 16029],\n",
      "        [    0,     0,     0,  ...,    14,  1976,    27],\n",
      "        [    0,     0,     0,  ...,   568,  2039,   236]], dtype=torch.int32)\n",
      "Sample input: \n",
      " tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Define the LSTM Network Architecture\n",
    "- The layers are as follows:\n",
    "\n",
    "    - Tokenize : This is not a layer for LSTM network but a mandatory step of converting our words into tokens (integers)\n",
    "    - Embedding Layer: that converts our word tokens (integers) into embedding of specific size\n",
    "    - LSTM Layer: defined by hidden state dims and number of layers\n",
    "    - Fully Connected Layer: that maps output of LSTM layer to a desired output size\n",
    "    - Sigmoid Activation Layer: that turns all output values in a value between 0 and 1\n",
    "    - Output: Sigmoid output from the last timestep is considered as the final output of this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        #embedding and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        #sigmoid function\n",
    "        sig_out = self.sigmoid(out)\n",
    "        \n",
    "        # reshape to be batch size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimentLSTM(\n",
      "  (embedding): Embedding(181513, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = sentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\駿燊\\nlpPractice\\PyTorch_LSTM.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000041?line=29'>30</a>\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([each\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m h])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000041?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000041?line=32'>33</a>\u001b[0m output,h \u001b[39m=\u001b[39m model(inputs,h)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000041?line=34'>35</a>\u001b[0m \u001b[39m# calculate the loss and perform backprop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000041?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39msqueeze(), labels\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpPractice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\駿燊\\nlpPractice\\PyTorch_LSTM.ipynb Cell 30'\u001b[0m in \u001b[0;36msentimentLSTM.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000033?line=32'>33</a>\u001b[0m \u001b[39m#embedding and lstm_out\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000033?line=33'>34</a>\u001b[0m embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000033?line=34'>35</a>\u001b[0m lstm_out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embeds, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000033?line=36'>37</a>\u001b[0m \u001b[39m#stack up lstm outputs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E9%A7%BF%E7%87%8A/nlpPractice/PyTorch_LSTM.ipynb#ch0000033?line=37'>38</a>\u001b[0m lstm_out \u001b[39m=\u001b[39m lstm_out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpPractice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpPractice\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=758'>759</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=759'>760</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=760'>761</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=761'>762</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=762'>763</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=763'>764</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    <a href='file:///c%3A/Users/DBLAB2020/anaconda3/envs/nlpPractice/lib/site-packages/torch/nn/modules/rnn.py?line=764'>765</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "clip = 5\n",
    "epochs = 2\n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        output, val_h = model(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "            \n",
    "        accuracy = acc(output,labels)\n",
    "        val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        # torch.save(model.state_dict(), '../working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "675b29fc675cbe1d63ca22ab0c08758279a0113940d3a430b596872d384684a6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlpPractice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
